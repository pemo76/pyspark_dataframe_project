{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxQuSjMf63oM"
      },
      "source": [
        "# Spark DataFrames Project Exercise "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRl_OXf663oR"
      },
      "source": [
        "Let's get some quick practice with your new Spark DataFrame skills, you will be asked some basic questions about some stock market data, in this case Walmart Stock from the years 2012-2017. This exercise will just ask a bunch of questions, unlike the future machine learning exercises, which will be a little looser and be in the form of \"Consulting Projects\", but more on that later!\n",
        "\n",
        "For now, just answer the questions and complete the tasks below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e-wyGX263oR"
      },
      "source": [
        "#### Use the walmart_stock.csv file to Answer and complete the  tasks below!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtcODew97R2m",
        "outputId": "2bebf5e0-3782-44a4-d979-79cfd8071664"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 45 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 49.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=d32eb0fd59ded0414c6fea7bd7fa26fa404e088c9d106556d238cf58c5e4cd18\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "3NYne3207YDV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IX0WoZW63oS"
      },
      "source": [
        "#### Start a simple Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "8yBFaDtx63oS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Project1').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz3OYFvr63oT"
      },
      "source": [
        "#### Load the Walmart Stock CSV File, have Spark infer the data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "7VlwUcPX63oT"
      },
      "outputs": [],
      "source": [
        "df_spark = spark.read.csv('/content/walmart_stock.csv',header=True,inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHO2KRgB63oU"
      },
      "source": [
        "#### What are the column names?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_4dEqbH63oU",
        "outputId": "cb6e7abb-5360-4fd8-ae85-bda8d1c7ce49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_spark.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-8TFsD863oV"
      },
      "source": [
        "#### What does the Schema look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8w6FnOK63oW",
        "outputId": "ed6e125f-4e0c-41c3-976c-74ad6885cd91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_spark.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJOGinxK63oW"
      },
      "source": [
        "#### Print out the first 5 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uol1jBod63oX",
        "outputId": "d00ba5ca-8bb9-4d3f-d0dc-55d3f58ed1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(Date=datetime.datetime(2012, 1, 3, 0, 0), Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996)\n",
            "\n",
            "\n",
            "Row(Date=datetime.datetime(2012, 1, 4, 0, 0), Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475)\n",
            "\n",
            "\n",
            "Row(Date=datetime.datetime(2012, 1, 5, 0, 0), Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539)\n",
            "\n",
            "\n",
            "Row(Date=datetime.datetime(2012, 1, 6, 0, 0), Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922)\n",
            "\n",
            "\n",
            "Row(Date=datetime.datetime(2012, 1, 9, 0, 0), Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj Close=51.616215000000004)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for row in df_spark.head(5):\n",
        "  print(row)\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQVjhfp163oX"
      },
      "source": [
        "#### Use describe() to learn about the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-poYEyZj63oX",
        "outputId": "1e3e4685-634b-4a87-cac9-737020fc1437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_spark.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3omyhu9V63oY"
      },
      "source": [
        "## Bonus Question!\n",
        "#### There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that .describe() returns, we didn't cover how to do this exact formatting, but we covered something very similar. [Check this link for a hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast)\n",
        "\n",
        "If you get stuck on this, don't worry, just view the solutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjd60nQJ63oY",
        "outputId": "e3355fc2-3beb-4ddd-899e-8bccfb8293cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- summary: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- Close: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Adj Close: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_spark.describe().printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "id": "SUoShTjG63oY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import format_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6m3NLE0163oZ"
      },
      "outputs": [],
      "source": [
        "result =df_spark.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select(result['summary'],\n",
        "              format_number(result['open'].cast('float'),2).alias('open'),\n",
        "              format_number(result['High'].cast('float'),2).alias('High'),\n",
        "              format_number(result['Low'].cast('float'),2).alias('Low'),\n",
        "              format_number(result['Close'].cast('float'),2).alias('Close'),\n",
        "              result['Volume'].cast('int').alias('Volume')\n",
        "              ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niOHZRlqIAwu",
        "outputId": "7f7770c0-8c94-4237-a99c-8224dccc6ecc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+--------+--------+--------+--------+\n",
            "|summary|    open|    High|     Low|   Close|  Volume|\n",
            "+-------+--------+--------+--------+--------+--------+\n",
            "|  count|1,258.00|1,258.00|1,258.00|1,258.00|    1258|\n",
            "|   mean|   72.36|   72.84|   71.92|   72.39| 8222093|\n",
            "| stddev|    6.77|    6.77|    6.74|    6.76| 4519780|\n",
            "|    min|   56.39|   57.06|   56.30|   56.42| 2094900|\n",
            "|    max|   90.80|   90.97|   89.25|   90.47|80898100|\n",
            "+-------+--------+--------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysziGWyn63oZ"
      },
      "source": [
        "#### Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feutHAoa63oZ",
        "outputId": "3fc089cc-d85d-488e-d849-cbac9c5678ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            HV Ratio|\n",
            "+--------------------+\n",
            "|4.819714653321546E-6|\n",
            "|6.290848613094555E-6|\n",
            "|4.669412994783916E-6|\n",
            "|7.367338463826307E-6|\n",
            "|8.915604778943901E-6|\n",
            "|8.644477436914568E-6|\n",
            "|9.351828421515645E-6|\n",
            "| 8.29141562102703E-6|\n",
            "|7.712212102001476E-6|\n",
            "|7.071764823529412E-6|\n",
            "|1.015495466386981E-5|\n",
            "|6.576354146362592...|\n",
            "| 5.90145296180676E-6|\n",
            "|8.547679455011844E-6|\n",
            "|8.420709512685392E-6|\n",
            "|1.041448341728929...|\n",
            "|8.316075414862431E-6|\n",
            "|9.721183814992126E-6|\n",
            "|8.029436027707578E-6|\n",
            "|6.307432259386365E-6|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "new_df = df_spark.withColumn(\"HV Ratio\",df_spark['High']/df_spark['Volume'])\n",
        "new_df.select(\"HV Ratio\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2jXDOz-63oZ"
      },
      "source": [
        "#### What day had the Peak High in Price?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbYGmAlr63oa",
        "outputId": "26cd2e76-68fd-4330-fc29-c6d64197c2cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2015, 1, 13, 0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "df_spark.orderBy(df_spark['High'].desc()).head(1)[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agRLZz6J63oa"
      },
      "source": [
        "#### What is the mean of the Close column?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP0VltNS63oa",
        "outputId": "7817ee7f-5247-418a-f7a7-86de53861f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       avg(Close)|\n",
            "+-----------------+\n",
            "|72.38844998012726|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import mean\n",
        "df_spark.select(mean(\"Close\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0ObnLLI63oa"
      },
      "source": [
        "#### What is the max and min of the Volume column?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": true,
        "id": "z3eM6Ox163oa"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import max,min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFxg4Zj763ob",
        "outputId": "8106aada-1a85-4464-a893-7563c16fbe0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|max(Volume)|min(Volume)|\n",
            "+-----------+-----------+\n",
            "|   80898100|    2094900|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_spark.select(max(\"Volume\"),min(\"Volume\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFY8IfdV63ob"
      },
      "source": [
        "#### How many days was the Close lower than 60 dollars?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "POpTZTAfLApf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGjbpkO_63ob",
        "outputId": "b7551373-03a8-4196-9145-2fe32ff9725b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "df_spark.filter('Close <60').count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK9dWtES63ob"
      },
      "source": [
        "#### What percentage of the time was the High greater than 80 dollars ?\n",
        "#### In other words, (Number of Days High>80)/(Total Days in the dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtuNqXDf63oc",
        "outputId": "c6157c0a-da7b-40bf-cc93-b84c90774f72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.141494435612083"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "df_spark.filter('High >80').count() / df_spark.count() *100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqo0aVsp63oc"
      },
      "source": [
        "#### What is the Pearson correlation between High and Volume?\n",
        "#### [Hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameStatFunctions.corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G4RuxxN63oc",
        "outputId": "1f9025d8-e87f-4c50-85af-99741e21ebfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "| corr(HIgh, Volume)|\n",
            "+-------------------+\n",
            "|-0.3384326061737161|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import corr\n",
        "df_spark.select(corr(\"HIgh\",\"Volume\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I6xGMIi63oc"
      },
      "source": [
        "#### What is the max High per year?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "kr9BlmcZ63oc"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import year\n",
        "year_df = df_spark.withColumn(\"Year\",year(df_spark[\"Date\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "year_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iKYF8pVNGPg",
        "outputId": "03a54fae-ed4f-405b-b319-3f3d9c57f1af"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+----+\n",
            "|               Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|Year|\n",
            "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+----+\n",
            "|2012-01-03 00:00:00|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|2012|\n",
            "|2012-01-04 00:00:00|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|2012|\n",
            "|2012-01-05 00:00:00|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|2012|\n",
            "|2012-01-06 00:00:00|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|2012|\n",
            "|2012-01-09 00:00:00|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|2012|\n",
            "|2012-01-10 00:00:00|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|2012|\n",
            "|2012-01-11 00:00:00|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|2012|\n",
            "|2012-01-12 00:00:00|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|2012|\n",
            "|2012-01-13 00:00:00|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|2012|\n",
            "|2012-01-17 00:00:00|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|2012|\n",
            "|2012-01-18 00:00:00|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|2012|\n",
            "|2012-01-19 00:00:00|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|2012|\n",
            "|2012-01-20 00:00:00|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996|2012|\n",
            "|2012-01-23 00:00:00|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|2012|\n",
            "|2012-01-24 00:00:00|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|2012|\n",
            "|2012-01-25 00:00:00|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|2012|\n",
            "|2012-01-26 00:00:00|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|2012|\n",
            "|2012-01-27 00:00:00|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|2012|\n",
            "|2012-01-30 00:00:00|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|2012|\n",
            "|2012-01-31 00:00:00|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|2012|\n",
            "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_df = year_df.groupBy(\"Year\").max()"
      ],
      "metadata": {
        "id": "3uG9feJNMbVD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2bKC9roNTcX",
        "outputId": "1a7fd0f6-2453-49c7-9a25-f788ecc03f06"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------------+---------+---------+----------+-----------+-----------------+---------+\n",
            "|Year|        max(Open)|max(High)| max(Low)|max(Close)|max(Volume)|   max(Adj Close)|max(Year)|\n",
            "+----+-----------------+---------+---------+----------+-----------+-----------------+---------+\n",
            "|2015|        90.800003|90.970001|    89.25| 90.470001|   80898100|84.91421600000001|     2015|\n",
            "|2013|        81.209999|81.370003|    80.82| 81.209999|   25683700|        73.929868|     2013|\n",
            "|2014|87.08000200000001|88.089996|86.480003| 87.540001|   22812400|81.70768000000001|     2014|\n",
            "|2012|        77.599998|77.599998|76.690002| 77.150002|   38007300|        68.568371|     2012|\n",
            "|2016|             74.5|75.190002|73.629997| 74.300003|   35076700|        73.233524|     2016|\n",
            "+----+-----------------+---------+---------+----------+-----------+-----------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_df.select('Year','max(High)').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8PXVrYSMnBH",
        "outputId": "46df1a25-0a63-48e2-a93b-85b70d4d7a29"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|Year|max(High)|\n",
            "+----+---------+\n",
            "|2015|90.970001|\n",
            "|2013|81.370003|\n",
            "|2014|88.089996|\n",
            "|2012|77.599998|\n",
            "|2016|75.190002|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq6w4JS863od"
      },
      "source": [
        "#### What is the average Close for each Calendar Month?\n",
        "#### In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "bTXff4Dx63od"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import month"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "month_df = df_spark.withColumn(\"Month\",month(\"Date\"))"
      ],
      "metadata": {
        "id": "ioQLhgwNNqWE"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "month_avg = month_df.select(['Month','Close']).groupBy('Month').mean()"
      ],
      "metadata": {
        "id": "au1eLM3FNqG6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "month_avg.select(\"Month\",\"avg(Close)\").orderBy('Month').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXJe4qgBPWrc",
        "outputId": "5655ef44-0edf-4548-c9c3-331873c2fa89"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|Month|       avg(Close)|\n",
            "+-----+-----------------+\n",
            "|    1|71.44801958415842|\n",
            "|    2|  71.306804443299|\n",
            "|    3|71.77794377570092|\n",
            "|    4|72.97361900952382|\n",
            "|    5|72.30971688679247|\n",
            "|    6| 72.4953774245283|\n",
            "|    7|74.43971943925233|\n",
            "|    8|73.02981855454546|\n",
            "|    9|72.18411785294116|\n",
            "|   10|71.57854545454543|\n",
            "|   11| 72.1110893069307|\n",
            "|   12|72.84792478301885|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Spark DataFrames Project Exercise.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}